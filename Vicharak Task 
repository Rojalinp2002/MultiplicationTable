#include <iostream>
#include <string>
#include <cctype>
#include <vector>
#include <unordered_map>

enum class TokenType {
    KEYWORD,
    IDENTIFIER,
    INTEGER,
    OPERATOR,
    DELIMITER,
    EOF_TOKEN,
    INVALID
};

struct Token {
    TokenType type;
    std::string value;

    Token(TokenType type, const std::string& value) : type(type), value(value) {}
};

class Lexer {
public:
    Lexer(const std::string& input) : input(input), index(0) {}

    std::vector<Token> tokenize() {
        std::vector<Token> tokens;
        while (index < input.length()) {
            char current = input[index];

            if (isspace(current)) {
                index++;
            } else if (isalpha(current)) {
                tokens.push_back(readIdentifierOrKeyword());
            } else if (isdigit(current)) {
                tokens.push_back(readInteger());
            } else if (current == '+' || current == '-' || current == '*' || current == '/' || current == '=' || current == '<' || current == '>') {
                tokens.push_back(readOperator());
            } else if (current == ';' || current == ',' || current == '(' || current == ')') {
                tokens.push_back(readDelimiter());
            } else {
                tokens.push_back(Token(TokenType::INVALID, std::string(1, current)));
                index++;
            }
        }
        tokens.push_back(Token(TokenType::EOF_TOKEN, ""));
        return tokens;
    }

private:
    std::string input;
    size_t index;

    std::unordered_map<std::string, TokenType> keywords = {
        {"if", TokenType::KEYWORD},
        {"else", TokenType::KEYWORD},
        {"while", TokenType::KEYWORD},
        {"int", TokenType::KEYWORD}
    };

    Token readIdentifierOrKeyword() {
        size_t start = index;
        while (index < input.length() && (isalnum(input[index]) || input[index] == '_')) {
            index++;
        }
        std::string value = input.substr(start, index - start);
        if (keywords.find(value) != keywords.end()) {
            return Token(TokenType::KEYWORD, value);
        } else {
            return Token(TokenType::IDENTIFIER, value);
        }
    }

    Token readInteger() {
        size_t start = index;
        while (index < input.length() && isdigit(input[index])) {
            index++;
        }
        return Token(TokenType::INTEGER, input.substr(start, index - start));
    }

    Token readOperator() {
        char op = input[index++];
        return Token(TokenType::OPERATOR, std::string(1, op));
    }

    Token readDelimiter() {
        char delimiter = input[index++];
        return Token(TokenType::DELIMITER, std::string(1, delimiter));
    }
};

void printTokens(const std::vector<Token>& tokens) {
    for (const auto& token : tokens) {
        switch (token.type) {
            case TokenType::KEYWORD:
                std::cout << "Keyword: " << token.value << std::endl;
                break;
            case TokenType::IDENTIFIER:
                std::cout << "Identifier: " << token.value << std::endl;
                break;
            case TokenType::INTEGER:
                std::cout << "Integer: " << token.value << std::endl;
                break;
            case TokenType::OPERATOR:
                std::cout << "Operator: " << token.value << std::endl;
                break;
            case TokenType::DELIMITER:
                std::cout << "Delimiter: " << token.value << std::endl;
                break;
            case TokenType::EOF_TOKEN:
                std::cout << "End of File" << std::endl;
                break;
            case TokenType::INVALID:
                std::cout << "Invalid Token: " << token.value << std::endl;
                break;
        }
    }
}

int main() {
    std::string input = "int main() { int x = 10; if (x > 5) { x = x + 1; } }";
    Lexer lexer(input);

    std::vector<Token> tokens = lexer.tokenize();
    printTokens(tokens);

    return 0;
}